{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worthy-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import matplotlib.pyplot as plt\n",
    "# from scipy.misc import imsave\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import time\n",
    "# import cv2 as cv\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "obvious-rogers",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FCN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FCN_input (InputLayer)       [(None, 512, 512, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 512, 512, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 512, 512, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512, 512, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 512, 512, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512, 512, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 512, 512, 64)      4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512, 512, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 512, 512, 64)      4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512, 512, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 512, 512, 3)       195       \n",
      "=================================================================\n",
      "Total params: 148,835\n",
      "Trainable params: 148,387\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#load trained model\n",
    "FCN_original = keras.models.load_model('C:/Users/MrLin/Documents/Experiments/Fine_Scale_Autoencoder_Lichen/FCN512_savemodel.h5')\n",
    "\n",
    "def process_image(imgtensor):\n",
    "    imgtensor *= (1/255)\n",
    "    return imgtensor\n",
    "\n",
    "def deprocess_image(imgtensor):\n",
    "    imgtensor *= 255\n",
    "    imgtensor = np.clip(imgtensor, 0, 255)\n",
    "    imgtensor = np.uint8(imgtensor)\n",
    "    return imgtensor\n",
    "\n",
    "FCN_original.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "casual-category",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 files belonging to 1 classes.\n",
      "tf.Tensor(18, shape=(), dtype=int64)\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node FCN/conv2d/Relu (defined at <ipython-input-7-2c01ae5a9457>:13) ]] [Op:__inference_predict_function_1390]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2c01ae5a9457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#PREDICT images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mxhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFCN_original\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_dataset_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mxhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    844\u001b[0m               *args, **kwds)\n\u001b[0;32m    845\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node FCN/conv2d/Relu (defined at <ipython-input-7-2c01ae5a9457>:13) ]] [Op:__inference_predict_function_1390]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "# plot reconstructions\n",
    "\n",
    "datadir = 'C:/Users/MrLin/Documents/Experiments/Fine_Scale_Autoencoder_Lichen/Results/test_images/input' #your images must be inside a subfolder in the last folder of datadir. if using multiple classes, put each class in one subfolder of the train_data folder. also put validation and test sets each in their own subfolder at the same level as the training folder\n",
    "#create images dataset\n",
    "img_dataset_validation = image_dataset_from_directory(\n",
    "    datadir, shuffle = False, image_size=(512, 512), batch_size=1, color_mode='rgb', validation_split=None, label_mode=None)\n",
    "\n",
    "#rescale pixel values\n",
    "img_dataset_validation = img_dataset_validation.map(process_image)\n",
    "print(img_dataset_validation.cardinality()) #set this value as the number of validation steps in .fit\n",
    "\n",
    "#PREDICT images \n",
    "xhat = FCN_original.predict(img_dataset_validation, steps=18)\n",
    "\n",
    "xhat = deprocess_image(xhat)\n",
    "# xhat = xhat.astype(np.uint8)\n",
    "xhat.shape\n",
    "for j in range(18):\n",
    "    pil_im = Image.fromarray(xhat[j,:,:,:],mode='RGB') # convert each slice of predicted batch to a PIL image object  \n",
    "    pil_im.save('C:/Users/MrLin/Documents/Experiments/Fine_Scale_Autoencoder_Lichen/Results/test_images/reconstructions/New folder/pred_' + f'{j+0:02}' + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "second-genealogy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv2d': <tf.Tensor 'conv2d/Relu:0' shape=(None, 512, 512, 32) dtype=float32>,\n",
       " 'conv2d_1': <tf.Tensor 'conv2d_1/Relu:0' shape=(None, 512, 512, 32) dtype=float32>,\n",
       " 'batch_normalization': <tf.Tensor 'batch_normalization/cond/Identity:0' shape=(None, 512, 512, 32) dtype=float32>,\n",
       " 'conv2d_2': <tf.Tensor 'conv2d_2/Relu:0' shape=(None, 512, 512, 64) dtype=float32>,\n",
       " 'conv2d_3': <tf.Tensor 'conv2d_3/Relu:0' shape=(None, 512, 512, 64) dtype=float32>,\n",
       " 'batch_normalization_1': <tf.Tensor 'batch_normalization_1/cond/Identity:0' shape=(None, 512, 512, 64) dtype=float32>,\n",
       " 'conv2d_4': <tf.Tensor 'conv2d_4/Relu:0' shape=(None, 512, 512, 64) dtype=float32>,\n",
       " 'conv2d_5': <tf.Tensor 'conv2d_5/Relu:0' shape=(None, 512, 512, 64) dtype=float32>,\n",
       " 'batch_normalization_2': <tf.Tensor 'batch_normalization_2/cond/Identity:0' shape=(None, 512, 512, 64) dtype=float32>,\n",
       " 'conv2d_6': <tf.Tensor 'conv2d_6/Relu:0' shape=(None, 512, 512, 64) dtype=float32>,\n",
       " 'conv2d_7': <tf.Tensor 'conv2d_7/Relu:0' shape=(None, 512, 512, 64) dtype=float32>,\n",
       " 'batch_normalization_3': <tf.Tensor 'batch_normalization_3/cond/Identity:0' shape=(None, 512, 512, 64) dtype=float32>,\n",
       " 'conv2d_8': <tf.Tensor 'conv2d_8/Relu:0' shape=(None, 512, 512, 3) dtype=float32>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a dict of models/layers that can be easily accessed to get intermediate outputs\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in FCN_original.layers[1:]]) #we wont need the output of input layer\n",
    "outputs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "turkish-destination",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 203 files belonging to 1 classes.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(203, 451)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXTRACT GLOBAL AVG POOLING BASED FEATURE VECTORS FOR EACH IMAGE AND STORE IN AN ARRAY\n",
    "# datadir = 'D:/Datasets/Lichen_partitioned' #your images must be inside a subfolder in the last folder of datadir. if using multiple classes, put each class in one subfolder of the train_data folder. also put validation and test sets each in their own subfolder at the same level as the training folder\n",
    "datadir = 'D:/Datasets/Lichen_partitioned_manualclusters'\n",
    "#create images dataset\n",
    "img_dataset = image_dataset_from_directory(\n",
    "    datadir, shuffle = False, image_size=(512, 512), batch_size=1, color_mode='rgb', validation_split=None, label_mode=None)\n",
    "\n",
    "def get_GAP(layer_output):\n",
    "    return K.mean(K.batch_flatten(K.permute_dimensions(layer_output,(2,0,1))), axis=1)\n",
    "    \n",
    "GAP_vector = []\n",
    "# cycle through all non-batch norm layers and concat the GAP into one long vector with length equal to the number of channels in the entire network\n",
    "for name in outputs_dict:\n",
    "    if name.find('batch_normalization') == -1: # skip batch norm layers\n",
    "        layer_output = outputs_dict[name]\n",
    "        GAP_vector.append(get_GAP(layer_output[0]))\n",
    "    \n",
    "GAP_vector = K.concatenate(GAP_vector, axis=0)\n",
    "    \n",
    "#create K.function mapping the input img to GAP feature vector\n",
    "inp = FCN_original.input\n",
    "image2GAP_vector = K.function([inp], GAP_vector) #\n",
    "\n",
    "Nimgs = 203 #4322\n",
    "img_dataset_sample = img_dataset.take(Nimgs)\n",
    "dat = np.zeros(shape=(Nimgs,451))\n",
    "j=0\n",
    "for img in img_dataset_sample:\n",
    "    print(j)\n",
    "    GAP_j = image2GAP_vector([img]) # the first moment a concrete array enters \n",
    "    dat[j,:]=GAP_j\n",
    "    j+=1\n",
    "# dat = np.array(dat)\n",
    "# dat\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "married-virtue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 16 nearest neighbors...\n",
      "[t-SNE] Indexed 203 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 203 samples in 0.017s...\n",
      "[t-SNE] Computed conditional probabilities for sample 203 / 203\n",
      "[t-SNE] Mean sigma: 331.952780\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 229.871994\n",
      "[t-SNE] KL divergence after 1000 iterations: 1.302042\n"
     ]
    }
   ],
   "source": [
    "# embed GAP vectors in 2D\n",
    "from sklearn.manifold import TSNE\n",
    "p_data_emb = TSNE(n_components=2, \n",
    "                  perplexity=5.0, \n",
    "                  early_exaggeration=24.0, \n",
    "                  learning_rate=200.0, \n",
    "                  n_iter=1000, \n",
    "                  n_iter_without_progress = 51,\n",
    "                  metric='euclidean', \n",
    "                  init='pca', \n",
    "                  verbose=1).fit(dat)\n",
    "p_data_emb.embedding_.shape\n",
    "emb = p_data_emb.embedding_ #if using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the embedding\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "images_dir = 'D:/Datasets/Lichen_partitioned_manualclusters/New folder'\n",
    "file_list = listdir(images_dir)\n",
    "\n",
    "\n",
    "fig2 = go.Figure(data=[go.Scatter(\n",
    "    name='training images',\n",
    "    x=emb[:,0],\n",
    "    y=emb[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4, color='black',symbol='circle')\n",
    ")])\n",
    "\n",
    "fig2.update_layout(\n",
    "    width=2000,\n",
    "    height=1800,\n",
    "                \n",
    "    scene_xaxis = dict(tickfont=dict(size=12),\n",
    "                title_font_size=20),\n",
    "    scene_yaxis = dict(tickfont=dict(size=12),\n",
    "                title_font_size=20),\n",
    ")\n",
    "\n",
    "mini_image_size=700\n",
    "for j, filename in enumerate(file_list):\n",
    "    lichen = Image.open('D:/Datasets/Lichen_partitioned_manualclusters/New folder/' + filename)\n",
    "    fig2.add_layout_image(\n",
    "            dict(\n",
    "                source=lichen,\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x=emb[j,0],\n",
    "                y=emb[j,1],\n",
    "                sizex=mini_image_size,\n",
    "                sizey=mini_image_size,\n",
    "                sizing=\"contain\",\n",
    "                opacity=1,\n",
    "                layer=\"above\")\n",
    "    )\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "assisted-sword",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_1.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_10.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_11.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_12.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_13.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_14.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_15.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_16.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_17.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_18.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_19.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_2.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_20.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_21.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_22.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_23.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_24.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_25.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_26.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_27.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_28.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_29.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_3.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_30.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_31.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_32.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_33.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_34.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_35.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_36.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_37.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_38.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_39.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_4.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_40.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_41.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_42.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_43.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_44.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_45.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_5.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_6.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_7.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_8.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I145_9.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_1.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_10.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_11.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_12.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_13.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_14.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_15.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_16.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_17.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_18.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_19.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_2.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_20.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_21.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_22.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_23.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_24.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_25.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_26.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_27.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_28.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_29.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_3.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_30.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_31.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_32.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_33.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_34.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_35.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_36.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_37.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_38.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_39.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_4.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_40.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_41.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_42.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_43.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_44.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_45.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_46.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_47.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_48.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_49.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_5.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_50.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_51.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_52.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_53.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_54.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_55.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_56.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_6.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_7.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_8.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I1_9.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_1.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_10.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_100.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_101.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_102.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_11.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_12.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_13.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_14.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_15.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_16.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_17.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_18.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_19.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_2.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_20.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_21.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_22.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_23.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_24.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_25.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_26.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_27.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_28.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_29.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_3.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_30.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_31.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_32.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_33.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_34.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_35.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_36.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_37.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_38.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_39.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_4.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_40.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_41.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_42.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_43.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_44.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_45.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_46.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_47.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_48.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_49.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_5.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_50.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_51.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_52.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_53.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_54.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_55.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_56.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_57.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_58.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_59.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_6.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_60.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_61.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_62.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_63.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_64.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_65.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_66.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_67.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_68.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_69.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_7.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_70.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_71.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_72.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_73.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_74.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_75.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_76.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_77.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_78.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_79.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_8.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_80.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_81.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_82.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_83.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_84.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_85.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_86.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_87.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_88.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_89.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_9.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_90.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_91.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_92.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_93.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_94.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_95.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_96.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_97.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_98.png\n",
      "D:/Datasets/Lichen_partitioned_manualclusters/New folder/pat_I4_99.png\n"
     ]
    }
   ],
   "source": [
    "# test code. (not needed for algorithm)\n",
    "from os import listdir\n",
    "images_dir = 'D:/Datasets/Lichen_partitioned_manualclusters/New folder'\n",
    "file_list = listdir(images_dir)\n",
    "\n",
    "for filename in file_list:\n",
    "    print('D:/Datasets/Lichen_partitioned_manualclusters/New folder/' + filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
